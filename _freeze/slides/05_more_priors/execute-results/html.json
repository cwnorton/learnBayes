{
  "hash": "071b164f7e906963b2e0faac6e4ba9db",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Introduction to Bayesian Linear Models\"\nsubtitle: \"05 - More on priors\"\nauthor: \"Stefano Coretta\"\ninstitute: \"University of Edinburgh\"\neditor: source\nformat:\n  mono-light-revealjs:\n    theme: [default, custom.scss]\n    history: false\nfilters:\n  - tachyonsextra\nexecute: \n  echo: true\n---\n\n\n\n::: {.cell}\n\n:::\n\n\n## Interactions: priors\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_prior(\n  RT ~ IsWord * PhonLev,\n  family = lognormal,\n  data = mald\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  prior     class                coef group resp dpar nlpar lb\n                 (flat)         b                                             \n                 (flat)         b         IsWordFALSE                         \n                 (flat)         b IsWordFALSE:PhonLev                         \n                 (flat)         b             PhonLev                         \n student_t(3, 6.9, 2.5) Intercept                                             \n   student_t(3, 0, 2.5)     sigma                                            0\n ub       source\n         default\n    (vectorized)\n    (vectorized)\n    (vectorized)\n         default\n         default\n```\n\n\n:::\n:::\n\n\n## Interactions: priors with no intercept\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_prior(\n  RT ~ 0 + IsWord + IsWord:PhonLev,\n  family = lognormal,\n  data = mald\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                prior class                coef group resp dpar nlpar lb ub\n               (flat)     b                                                \n               (flat)     b         IsWordFALSE                            \n               (flat)     b IsWordFALSE:PhonLev                            \n               (flat)     b          IsWordTRUE                            \n               (flat)     b  IsWordTRUE:PhonLev                            \n student_t(3, 0, 2.5) sigma                                            0   \n       source\n      default\n (vectorized)\n (vectorized)\n (vectorized)\n (vectorized)\n      default\n```\n\n\n:::\n:::\n\n\n## The model and the priors\n\n$$\n\\begin{align}\n\\text{RT_i} & \\sim LN(\\mu_i, \\sigma) \\\\\nlog(\\mu_i) & = \\beta_{1_{IW[i]}} + \\beta_{2_{IW[i]}} \\cdot \\text{PhonLev}_i \\\\\n\\beta_{1_{IW[T]}} & \\sim Gaussian(\\mu_1, \\sigma_1) \\\\\n\\beta_{1_{IW[F]}} & \\sim Gaussian(\\mu_2, \\sigma_2) \\\\\n\\beta_{2_{IW[T]}} & \\sim Gaussian(\\mu_3, \\sigma_3) \\\\\n\\beta_{2_{IW[F]}} & \\sim Gaussian(\\mu_4, \\sigma_4) \\\\\n\\sigma & \\sim Cauchy_{+}(0, \\sigma_5)\n\\end{align}\n$$\n\n## Centring numeric predictors\n\n::: box-note\n\n- `PhonLev` is numeric, so for the $\\beta_{1_{IW[i]}}$ priors, we need to think about the mean log-RT when `PhonLev` is 0.\n\n- Let's centre `PhonLev`, so that we need to think about the mean log_RT when `PhonLev` is at its mean.\n\n:::\n\n. . .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(mald$PhonLev)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.092255\n```\n\n\n:::\n\n```{.r .cell-code}\nmald <- mald |> \n  mutate(PhonLev_c = PhonLev - mean(PhonLev))\n```\n:::\n\n\n## The model (revised)\n\n$$\n\\begin{align}\n\\text{RT_i} & \\sim LN(\\mu_i, \\sigma) \\\\\nlog(\\mu_i) & = \\beta_{1_{IW[i]}} + \\beta_{2_{IW[i]}} \\cdot (\\text{PhonLev}_i - mean(\\text{PhonLev})) \\\\\n\\end{align}\n$$\n## Prior for $\\beta_i$\n\n::: box-note\n-   Let's say again that the mean RT is between 500 and 2500 ms at 95% confidence. Now let's log these.\n  \n  - In logs: `log(500) = 6.2` and `log(2500) = 7.8`\n\n-   Get $\\mu_1$ and $\\mu_2$\n\n    -   `mean(c(6.2, 7.8))` = 7\n\n-   Get $\\sigma_1$ and $\\sigma_2$\n\n    -   `(7.8 - 7) / 2` = 0.4 (let's round to 0.5)\n:::\n\n. . .\n\n$$\n\\begin{align}\n\\text{RT_i} & \\sim LN(\\mu_i, \\sigma) \\\\\nlog(\\mu_i) & = \\beta_{1_{IW[i]}} + \\beta_{2_{IW[i]}} \\cdot (\\text{PhonLev}_i - mean(\\text{PhonLev})) \\\\\n\\beta_{1_{IW[T]}} & \\sim Gaussian(7, 0.5) \\\\\n\\beta_{1_{IW[F]}} & \\sim Gaussian(7, 0.5) \\\\\n\\end{align}\n$$\n\n## Prior for $\\beta_2$\n\n::: box-note\n\n- Let's assume no expectation about the effect of `PhonLev`, apart from that can be negative or positive or null, and not very large.\n\n- $Gaussian(0, 0.1)$.\n\n  - This means we are 95% \"confident\" that the effect of `PhonLev` on log-RTs is between -0.2 and +0.2 for each unit increase of `PhonLev`.\n  \n  - `800 * exp(0.2) = 800 * 1.22 = 976` i.e. a (`976 - 800 =`) 176 ms increase per unit increase. (Also `800 * (1.22 - 1)`).\n\n:::\n\n. . .\n\n$$\n\\begin{align}\n\\text{RT_i} & \\sim LN(\\mu_i, \\sigma) \\\\\nlog(\\mu_i) & = \\beta_{1_{IW[i]}} + \\beta_{2_{IW[i]}} \\cdot (\\text{PhonLev}_i - mean(\\text{PhonLev})) \\\\\n\\beta_{1_{IW[T]}} & \\sim Gaussian(7, 0.5) \\\\\n\\beta_{1_{IW[F]}} & \\sim Gaussian(7, 0.5) \\\\\n\\beta_{2_{IW[T]}} & \\sim Gaussian(0, 0.1) \\\\\n\\beta_{2_{IW[F]}} & \\sim Gaussian(0, 0.1) \\\\\n\\end{align}\n$$\n\n## Prior predictive checks\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrm_4b_priors <- c(\n  prior(normal(7, 0.5), class = b, coef = IsWordFALSE),\n  prior(normal(7, 0.5), class = b, coef = IsWordTRUE),\n  prior(normal(0, 0.1), class = b, coef = `IsWordFALSE:PhonLev_c`),\n  prior(normal(0, 0.1), class = b, coef = `IsWordTRUE:PhonLev_c`),\n  prior(cauchy(0, 0.02), class = sigma)\n)\n\nbrm_4b_priorpp <- brm(\n  RT ~ 0 + IsWord + IsWord:PhonLev_c,\n  family = lognormal,\n  prior = brm_4b_priors,\n  data = mald,\n  sample_prior = \"only\",\n  cores = 4,\n  file = \"data/cache/brm_4b_priorpp\",\n  seed = my_seed\n)\n```\n:::\n\n\n## Prior predictive checks plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconditional_effects(brm_4b_priorpp, \"PhonLev_c:IsWord\")\n```\n\n::: {.cell-output-display}\n![](05_more_priors_files/figure-revealjs/brm-4b-priorpp-cond-1.png){width=960}\n:::\n:::\n\n\n## Run the model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrm_4b <- brm(\n  RT ~ 0 + IsWord + IsWord:PhonLev_c,\n  family = lognormal,\n  prior = brm_4b_priors,\n  data = mald,\n  cores = 4,\n  file = \"data/cache/brm_4b\",\n  seed = my_seed\n)\n```\n:::\n\n\n## Model summary\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrm_4b\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: lognormal \n  Links: mu = identity; sigma = identity \nFormula: RT ~ 0 + IsWord + IsWord:PhonLev_c \n   Data: mald (Number of observations: 3000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIsWordTRUE                6.85      0.01     6.84     6.86 1.00     4894\nIsWordFALSE               6.97      0.01     6.95     6.98 1.00     5171\nIsWordTRUE:PhonLev_c      0.03      0.01     0.02     0.04 1.00     5139\nIsWordFALSE:PhonLev_c     0.02      0.01     0.01     0.03 1.00     5870\n                      Tail_ESS\nIsWordTRUE                3055\nIsWordFALSE               3022\nIsWordTRUE:PhonLev_c      3388\nIsWordFALSE:PhonLev_c     3001\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.28      0.00     0.27     0.29 1.00     4232     3160\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\n## Posterior predictive checks\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_check(brm_4b, ndraws = 50)\n```\n\n::: {.cell-output-display}\n![](05_more_priors_files/figure-revealjs/brm-4b-pp-1.png){width=960}\n:::\n:::\n\n\n\n## Conditional posterior probabilies\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconditional_effects(brm_4b, \"PhonLev_c:IsWord\")\n```\n\n::: {.cell-output-display}\n![](05_more_priors_files/figure-revealjs/brm-4b-cond-1.png){width=960}\n:::\n:::\n",
    "supporting": [
      "05_more_priors_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
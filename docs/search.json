[
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Pre-workshop set-up",
    "section": "",
    "text": "Please, follow these instructions carefully to get ready at least one day before the workshop."
  },
  {
    "objectID": "setup.html#pre-requisites",
    "href": "setup.html#pre-requisites",
    "title": "Pre-workshop set-up",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nBefore installing the necessary software, make sure you have installed or updated the following software.\n\nThe latest version of R (https://cloud.r-project.org).\nThe latest version of RStudio (https://www.rstudio.com/products/rstudio/download/#download).\nYour operating system is up-to-date."
  },
  {
    "objectID": "setup.html#installation",
    "href": "setup.html#installation",
    "title": "Pre-workshop set-up",
    "section": "Installation",
    "text": "Installation\n\n\n\n\n\n\nImportant\n\n\n\nIf you have previously installed the C++ toolkit or if you have recently updated your OS, please follow these instructions to reinstall them.\n\n\nNow you will need to install a few packages and extra software.\nHere is an overview of what you will install:\n\nC++ toolchain.\nR packages: tidyverse, brms, tidybayes, extraDistr.\n\n\n1. Install the C++ toolchain\nThe package brms used in the workshop requires a working C++ toolchain to compile models.\n\nWindows\nFor Windows, follow the instructions here: https://cran.r-project.org/bin/windows/Rtools/rtools43/rtools.html\n\n\nmacOS\nFor macOS, open the Terminal and write the following line then press enter/return:\nxcode-select --install\nYouâ€™ll see a panel that asks you to install the Xcode Command Line Tools. Install them. Downloading and installation will take 30 to 60 minutes.\n\n\nLinux\nFor Linux, follow the instructions here: https://github.com/stan-dev/rstan/wiki/Configuring-C-Toolchain-for-Linux\n\n\n\n2. Install the R packages\nYou need to install the following packages:\ninstall.packages(c(\"tidyverse\", \"brms\", \"tidybayes\", \"extraDistr\"))\nIt will take several minutes to install the packages, depending on your system and configuration.\nIf after opening the Starter Kit in RStudio you get asked to install extra packages or software, please do so.\n\n\nCheck your installation\nRun the following in the RStudio Console:\nexample(stan_model, package = \"rstan\", run.dontrun = TRUE)\nIf you see some strange looking text printed in the Console and then fit and fit2 in the Environment, then you are sorted!"
  },
  {
    "objectID": "setup.html#troubleshooting",
    "href": "setup.html#troubleshooting",
    "title": "Pre-workshop set-up",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nIf you are having issues with installation, the best place to ask for help are:\n\nThe Stan Forums: https://discourse.mc-stan.org\nStackOverflow: https://stackoverflow.com\nAny search engine."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learn Bayesian Linear Models in R",
    "section": "",
    "text": "This is the website of the workshop learnBayes.\nPlease, check the Set-up page for important set-up instructions, before joining the workshop.\nThis workshop assumes you have a solid command of R and tidyverse packages, and familiarity with linear modelling (including interpreting coefficients and interactions, running linear models with binomial/Bernoulli family, aka logistic regression).\nIf you wish to revise any of these topics, we recommend the following resources:\n\nR for Data Science (online book).\nStatistics for Linguists in R (texbook)."
  },
  {
    "objectID": "slides/01_part_1.html#section",
    "href": "slides/01_part_1.html#section",
    "title": "Introduction to Bayesian Linear Models",
    "section": "",
    "text": "One model to rule them all. One model to find them. One model to bring them all And in the darkness bind them.\n\n\n\n(No, I couldnâ€™t translate â€˜modelâ€™ into Black Speech, alasâ€¦)"
  },
  {
    "objectID": "slides/01_part_1.html#now-enter-the-linear-model",
    "href": "slides/01_part_1.html#now-enter-the-linear-model",
    "title": "Introduction to Bayesian Linear Models",
    "section": "Now enter The Linear Model",
    "text": "Now enter The Linear Model"
  },
  {
    "objectID": "slides/01_part_1.html#now-enter-the-bayesian-linear-model",
    "href": "slides/01_part_1.html#now-enter-the-bayesian-linear-model",
    "title": "Introduction to Bayesian Linear Models",
    "section": "Now enter The Bayesian Linear Model",
    "text": "Now enter The Bayesian Linear Model"
  },
  {
    "objectID": "slides/01_part_1.html#all-about-the-bayes",
    "href": "slides/01_part_1.html#all-about-the-bayes",
    "title": "Introduction to Bayesian Linear Models",
    "section": "All about the Bayes",
    "text": "All about the Bayes\n\nWithin the NHST (Frequentist) framework, the main analysis output is:\n\nPoint estimates of predictorsâ€™ parameters (with standard error).\nP-values. ðŸ˜ˆ\n\n\n\n\nWithin the Bayesian framework, the main analysis output is:\n\nProbability distributions of predictorsâ€™ parameters."
  },
  {
    "objectID": "slides/01_part_1.html#an-example",
    "href": "slides/01_part_1.html#an-example",
    "title": "Introduction to Bayesian Linear Models",
    "section": "An example",
    "text": "An example\nMassive Auditory Lexical Decision (Tucker et al.Â 2019):\n\nMALD data set:\n\nAuditory Lexical Decision task with real and nonce English words.\nReaction times and accuracy.\nTotal 521 subjects; subset of 30 subjects, 100 observations each.\n\nLetâ€™s investigate the effect of mean phone-level Levenshtein distance and lexical status (word vs non-word).\n\n\n\n\n# A tibble: 3,000 Ã— 7\n   Subject Item         IsWord PhonLev    RT ACC       RT_log\n   &lt;chr&gt;   &lt;chr&gt;        &lt;fct&gt;    &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;      &lt;dbl&gt;\n 1 15345   nihnaxr      FALSE     5.84   945 correct     6.85\n 2 15345   skaep        FALSE     6.03  1046 incorrect   6.95\n 3 15345   grandparents TRUE     10.3    797 correct     6.68\n 4 15345   sehs         FALSE     5.88  2134 correct     7.67\n 5 15345   cousin       TRUE      5.78   597 correct     6.39\n 6 15345   blowup       TRUE      6.03   716 correct     6.57\n 7 15345   hhehrnzmaxn  FALSE     7.30  1985 correct     7.59\n 8 15345   mantic       TRUE      6.21  1591 correct     7.37\n 9 15345   notable      TRUE      6.82   620 correct     6.43\n10 15345   prowthihviht FALSE     7.68  1205 correct     7.09\n# â„¹ 2,990 more rows"
  },
  {
    "objectID": "slides/01_part_1.html#an-example-mald",
    "href": "slides/01_part_1.html#an-example-mald",
    "title": "Introduction to Bayesian Linear Models",
    "section": "An example: MALD",
    "text": "An example: MALD\n\nMassive Auditory Lexical Decision (MALD) data (Tucker et al.Â 2019):\n\nAuditory Lexical Decision task with real and nonce English words.\nReaction times and accuracy.\nTotal 521 subjects; subset of 30 subjects, 100 observations each.\n\n\n\n\nmald\n\n# A tibble: 3,000 Ã— 7\n   Subject Item         IsWord PhonLev    RT ACC       RT_log\n   &lt;chr&gt;   &lt;chr&gt;        &lt;fct&gt;    &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;      &lt;dbl&gt;\n 1 15345   nihnaxr      FALSE     5.84   945 correct     6.85\n 2 15345   skaep        FALSE     6.03  1046 incorrect   6.95\n 3 15345   grandparents TRUE     10.3    797 correct     6.68\n 4 15345   sehs         FALSE     5.88  2134 correct     7.67\n 5 15345   cousin       TRUE      5.78   597 correct     6.39\n 6 15345   blowup       TRUE      6.03   716 correct     6.57\n 7 15345   hhehrnzmaxn  FALSE     7.30  1985 correct     7.59\n 8 15345   mantic       TRUE      6.21  1591 correct     7.37\n 9 15345   notable      TRUE      6.82   620 correct     6.43\n10 15345   prowthihviht FALSE     7.68  1205 correct     7.09\n# â„¹ 2,990 more rows"
  },
  {
    "objectID": "slides/01_part_1.html#mald-phone-level-distance-and-lexical-status",
    "href": "slides/01_part_1.html#mald-phone-level-distance-and-lexical-status",
    "title": "Introduction to Bayesian Linear Models",
    "section": "MALD: phone-level distance and lexical status",
    "text": "MALD: phone-level distance and lexical status\nLetâ€™s investigate the effects of:\n\nPhonLev: mean phone-level Levenshtein distance.\nIsWord: lexical status (real word vs nonce word).\n\nOn:\n\nRT: reaction times.\nACC: accuracy."
  },
  {
    "objectID": "slides/01_part_1.html#mald-phone-level-distance-and-lexical-status-1",
    "href": "slides/01_part_1.html#mald-phone-level-distance-and-lexical-status-1",
    "title": "Introduction to Bayesian Linear Models",
    "section": "MALD: phone-level distance and lexical status",
    "text": "MALD: phone-level distance and lexical status\n\nmald |&gt; \n  ggplot(aes(PhonLev, RT)) +\n  geom_point(alpha = 0.1) +\n  geom_smooth(aes(colour = IsWord, fill = IsWord), method = \"lm\", formula = y ~ x) +\n  labs(\n    x = \"Phone-level distance\", y = \"RT (ms)\"\n  )"
  },
  {
    "objectID": "slides/01_part_1.html#a-frequentist-linear-model",
    "href": "slides/01_part_1.html#a-frequentist-linear-model",
    "title": "Introduction to Bayesian Linear Models",
    "section": "A frequentist linear model",
    "text": "A frequentist linear model\n\nlm_1 &lt;- lmer(\n  RT ~\n    PhonLev +\n    IsWord +\n    PhonLev:IsWord +\n    (1 | Subject),\n  data = mald\n)"
  },
  {
    "objectID": "slides/01_part_1.html#a-frequentist-linear-model-summary",
    "href": "slides/01_part_1.html#a-frequentist-linear-model-summary",
    "title": "Introduction to Bayesian Linear Models",
    "section": "A frequentist linear model: summary",
    "text": "A frequentist linear model: summary\n\nsummary(lm_1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: RT ~ PhonLev + IsWord + PhonLev:IsWord + (1 | Subject)\n   Data: mald\n\nREML criterion at convergence: 43232.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.5511 -0.5985 -0.2439  0.3185  5.6906 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Subject  (Intercept)  11032   105.0   \n Residual             104648   323.5   \nNumber of obs: 3000, groups:  Subject, 30\n\nFixed effects:\n                    Estimate Std. Error t value\n(Intercept)          754.965     49.687  15.195\nPhonLev               32.148      6.389   5.032\nIsWordFALSE          212.440     65.453   3.246\nPhonLev:IsWordFALSE  -11.620      9.076  -1.280\n\nCorrelation of Fixed Effects:\n            (Intr) PhonLv IWFALS\nPhonLev     -0.907              \nIsWordFALSE -0.647  0.690       \nPhL:IWFALSE  0.640 -0.705 -0.983"
  },
  {
    "objectID": "slides/01_part_1.html#a-frequentist-linear-model-plot-predictions",
    "href": "slides/01_part_1.html#a-frequentist-linear-model-plot-predictions",
    "title": "Introduction to Bayesian Linear Models",
    "section": "A frequentist linear model: plot predictions",
    "text": "A frequentist linear model: plot predictions\n\nggpredict(lm_1, terms = c(\"PhonLev\", \"IsWord\")) %&gt;%\n  plot()"
  },
  {
    "objectID": "slides/01_part_1.html#increase-model-complexity",
    "href": "slides/01_part_1.html#increase-model-complexity",
    "title": "Introduction to Bayesian Linear Models",
    "section": "Increase model complexity",
    "text": "Increase model complexity\nTry it yourself! Does it work?\n\nlm_2 &lt;- lmer(\n  RT ~\n    PhonLev +\n    IsWord +\n    PhonLev:IsWord +\n    (PhonLev + IsWord | Subject),\n  data = mald\n)\n\n\n\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.0430116 (tol = 0.002, component 1)"
  },
  {
    "objectID": "slides/01_part_1.html#section-1",
    "href": "slides/01_part_1.html#section-1",
    "title": "Introduction to Bayesian Linear Models",
    "section": "",
    "text": "Letâ€™s go Bayesian!"
  },
  {
    "objectID": "slides/01_part_1.html#a-bayesian-linear-model",
    "href": "slides/01_part_1.html#a-bayesian-linear-model",
    "title": "Introduction to Bayesian Linear Models",
    "section": "A Bayesian linear model",
    "text": "A Bayesian linear model\n\nbrm_1 &lt;- brm(\n  RT ~\n    PhonLev +\n    IsWord +\n    PhonLev:IsWord +\n    (PhonLev + IsWord | Subject),\n  data = mald,\n  family = gaussian()\n)"
  },
  {
    "objectID": "slides/01_part_1.html#a-bayesian-linear-model-summary",
    "href": "slides/01_part_1.html#a-bayesian-linear-model-summary",
    "title": "Introduction to Bayesian Linear Models",
    "section": "A Bayesian linear model: summary",
    "text": "A Bayesian linear model: summary\n\nbrm_1\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ PhonLev + IsWord + PhonLev:IsWord + (PhonLev + IsWord | Subject) \n   Data: mald (Number of observations: 3000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~Subject (Number of levels: 30) \n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(Intercept)                 98.77     33.49    44.25   177.00 1.01     1156\nsd(PhonLev)                    7.76      5.03     0.47    19.03 1.01      465\nsd(IsWordFALSE)               97.43     19.11    62.96   139.07 1.00     1804\ncor(Intercept,PhonLev)        -0.42      0.44    -0.93     0.68 1.00     1522\ncor(Intercept,IsWordFALSE)     0.53      0.27    -0.05     0.94 1.01      589\ncor(PhonLev,IsWordFALSE)      -0.36      0.44    -0.94     0.65 1.02      384\n                           Tail_ESS\nsd(Intercept)                  1261\nsd(PhonLev)                    1031\nsd(IsWordFALSE)                2778\ncor(Intercept,PhonLev)         2153\ncor(Intercept,IsWordFALSE)     1515\ncor(PhonLev,IsWordFALSE)       1057\n\nRegression Coefficients:\n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept             755.18     49.73   659.14   853.47 1.00     2234     1457\nPhonLev                31.94      6.60    18.84    44.46 1.00     2510     2759\nIsWordFALSE           208.11     67.09    75.56   339.99 1.00     2523     2380\nPhonLev:IsWordFALSE   -11.10      9.04   -28.42     6.65 1.00     2493     2788\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma   320.27      4.25   312.03   328.73 1.00     5408     2783\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "slides/01_part_1.html#lets-start-small",
    "href": "slides/01_part_1.html#lets-start-small",
    "title": "Introduction to Bayesian Linear Models",
    "section": "Letâ€™s start small",
    "text": "Letâ€™s start small\nTry it yourself!\n\nbrm_2 &lt;- brm(\n  # formula = outcome ~ predictor\n  RT ~ IsWord,\n  data = mald,\n  # Specify the distribution family of the outcome\n  family = gaussian()\n)\n\n\n\n\nThe model estimates the probability distributions of the effects of each predictor.\nTo do so, a sampling algorithm is used (Markov Chain Monte Carlo, MCMC)."
  },
  {
    "objectID": "slides/01_part_1.html#mcmc-what",
    "href": "slides/01_part_1.html#mcmc-what",
    "title": "Introduction to Bayesian Linear Models",
    "section": "MCMC what?",
    "text": "MCMC what?\nMarkov Chain Monte Carlo\n\nMCMC simulation: https://chi-feng.github.io/mcmc-demo/app.html\nMore on MCMC: http://elevanth.org/blog/2017/11/28/build-a-better-markov-chain/"
  },
  {
    "objectID": "slides/01_part_1.html#technicalities",
    "href": "slides/01_part_1.html#technicalities",
    "title": "Introduction to Bayesian Linear Models",
    "section": "Technicalities",
    "text": "Technicalities\n\nbrm_2 &lt;- brm(\n  RT ~ IsWord,\n  data = mald,\n  family = gaussian(),\n  \n  # TECHNICAL STUFF\n  # Save model output to file\n  file = \"./data/cache/brm_2.rds\",\n  # Number of MCMC chains\n  chains = 4, \n  # Number of iterations per chain\n  iter = 2000,\n  # Number of cores to use (one per chain)\n  cores = 4\n)\n\n\nYou can find out how many cores your laptop has with parallel::detectCores()."
  },
  {
    "objectID": "slides/01_part_1.html#a-bayesian-linear-model-plot-predictions",
    "href": "slides/01_part_1.html#a-bayesian-linear-model-plot-predictions",
    "title": "Introduction to Bayesian Linear Models",
    "section": "A Bayesian linear model: plot predictions",
    "text": "A Bayesian linear model: plot predictions\n\nconditional_effects(brm_1, effects = \"PhonLev:IsWord\")"
  },
  {
    "objectID": "slides/01_part_1.html#interpreting-the-summary",
    "href": "slides/01_part_1.html#interpreting-the-summary",
    "title": "Introduction to Bayesian Linear Models",
    "section": "Interpreting the summary",
    "text": "Interpreting the summary\n\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     981.35      8.73   964.38   998.37 1.00     3970     3032\nIsWordFALSE   132.87     12.63   107.91   156.87 1.00     3936     2937\n\n\n\nIntercept: There is a 95% probability that (based on model and data) the mean RT when the word is real is between 964 and 999 ms.\n\nThe probability distribution of the intercept has mean = 981 ms and SD = 9 (rounded).\n\n\n\n\nIsWordFALSE: At 95% confidence, the difference in RT between non-words and real words is between +109 and +158 ms (based on model and data).\n\nThe probability distribution of IsWordFALSE has mean = 133 ms and SD = 13 (rounded)."
  },
  {
    "objectID": "slides/01_part_1.html#interpreting-the-summary-1",
    "href": "slides/01_part_1.html#interpreting-the-summary-1",
    "title": "Introduction to Bayesian Linear Models",
    "section": "Interpreting the summary",
    "text": "Interpreting the summary\n\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     981.35      8.73   964.38   998.37 1.00     3970     3032\nIsWordFALSE   132.87     12.63   107.91   156.87 1.00     3936     2937\n\n\n\nIntercept: There is a 95% probability that (based on model and data) the mean RT when the word is real is between 964 and 999 ms.\n\nThe probability distribution of the intercept has mean = 981 ms and SD = 9 (rounded).\n\n\n\n\nIsWordFALSE: At 95% confidence, the difference in RT between non-words and real words is between +109 and +158 ms (based on model and data).\n\nThe probability distribution of IsWordFALSE has mean = 133 ms and SD = 13 (rounded)."
  },
  {
    "objectID": "slides/01_part_1.html#posterior-probabilities",
    "href": "slides/01_part_1.html#posterior-probabilities",
    "title": "Introduction to Bayesian Linear Models",
    "section": "Posterior probabilities",
    "text": "Posterior probabilities\n\n\nThese are posterior probability distributions.\nThey are always conditional on model and data.\nThe summary reports 95% Credible Intervals, but you can get other intervals too (there is nothing special about 95%).\n\n\n\n\nsummary(brm_2, prob = 0.8)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ IsWord \n   Data: mald (Number of observations: 3000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nIntercept     981.35      8.73   970.04   992.96 1.00     3970     3032\nIsWordFALSE   132.87     12.63   116.58   148.84 1.00     3936     2937\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nsigma   341.21      4.48   335.59   346.97 1.00     4223     3006\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "slides/01_part_1.html#lets-go-bayesian",
    "href": "slides/01_part_1.html#lets-go-bayesian",
    "title": "Introduction to Bayesian Linear Models",
    "section": "Letâ€™s go Bayesian!",
    "text": "Letâ€™s go Bayesian!\nâ€¦like the Bayesian Rats!"
  },
  {
    "objectID": "slides/01_part_1.html#the-model-summary",
    "href": "slides/01_part_1.html#the-model-summary",
    "title": "Introduction to Bayesian Linear Models",
    "section": "The model summary",
    "text": "The model summary\n\nbrm_2\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ IsWord \n   Data: mald (Number of observations: 3000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     981.35      8.73   964.38   998.37 1.00     3970     3032\nIsWordFALSE   132.87     12.63   107.91   156.87 1.00     3936     2937\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma   341.21      4.48   332.33   349.82 1.00     4223     3006\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "slides/01_part_1.html#quick-plot",
    "href": "slides/01_part_1.html#quick-plot",
    "title": "Introduction to Bayesian Linear Models",
    "section": "Quick plot",
    "text": "Quick plot\n\nplot(brm_2, combo = c(\"dens\", \"trace\"))"
  },
  {
    "objectID": "slides/01_part_1.html#extract-the-mcmc-draws",
    "href": "slides/01_part_1.html#extract-the-mcmc-draws",
    "title": "Introduction to Bayesian Linear Models",
    "section": "Extract the MCMC draws",
    "text": "Extract the MCMC draws\n\nbrm_2_draws &lt;- as_draws_df(brm_2)\nbrm_2_draws\n\n# A draws_df: 1000 iterations, 4 chains, and 6 variables\n   b_Intercept b_IsWordFALSE sigma Intercept lprior   lp__\n1          981           144   349      1052    -13 -21763\n2          989           131   343      1053    -13 -21761\n3          973           138   338      1040    -13 -21761\n4          983           126   337      1045    -13 -21761\n5          996           114   346      1052    -13 -21762\n6          966           156   339      1042    -13 -21762\n7          973           140   343      1042    -13 -21761\n8          996           127   344      1058    -13 -21762\n9          990           123   340      1050    -13 -21761\n10         970           145   341      1042    -13 -21761\n# ... with 3990 more draws\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}"
  },
  {
    "objectID": "slides/01_part_1.html#get-parameters",
    "href": "slides/01_part_1.html#get-parameters",
    "title": "Introduction to Bayesian Linear Models",
    "section": "Get parameters",
    "text": "Get parameters\nTo list all the names of the parameters in a model, use:\n\nvariables(brm_2)\n\n[1] \"b_Intercept\"   \"b_IsWordFALSE\" \"sigma\"         \"Intercept\"    \n[5] \"lprior\"        \"lp__\""
  },
  {
    "objectID": "slides/01_part_1.html#posterior-distributions",
    "href": "slides/01_part_1.html#posterior-distributions",
    "title": "Introduction to Bayesian Linear Models",
    "section": "Posterior distributions",
    "text": "Posterior distributions\nNow you can plot the draws using ggplot2.\n\nbrm_2_draws %&gt;%\n  ggplot(aes(b_Intercept)) +\n  stat_halfeye(fill = \"#214d65\", alpha = 0.8) +\n  scale_x_continuous() +\n  labs(title = \"Posterior distribution of Intercept\")"
  },
  {
    "objectID": "slides/01_part_1.html#posterior-distributions-output",
    "href": "slides/01_part_1.html#posterior-distributions-output",
    "title": "Introduction to Bayesian Linear Models",
    "section": "Posterior distributions",
    "text": "Posterior distributions"
  },
  {
    "objectID": "slides/02_diagnostics.html#mcmc-traces",
    "href": "slides/02_diagnostics.html#mcmc-traces",
    "title": "Introduction to Bayesian Linear Models",
    "section": "MCMC traces",
    "text": "MCMC traces\n\nplot(brm_2)"
  },
  {
    "objectID": "slides/02_diagnostics.html#mcmc-traces-bad",
    "href": "slides/02_diagnostics.html#mcmc-traces-bad",
    "title": "Introduction to Bayesian Linear Models",
    "section": "MCMC traces: bad",
    "text": "MCMC traces: bad\nAn example of bad MCMC chain mixing.\n\n\nPicture from https://www.rdatagen.net/post/diagnosing-and-dealing-with-estimation-issues-in-the-bayesian-meta-analysis/."
  },
  {
    "objectID": "slides/02_diagnostics.html#mcmc-traces-intercept",
    "href": "slides/02_diagnostics.html#mcmc-traces-intercept",
    "title": "Introduction to Bayesian Linear Models",
    "section": "MCMC traces: intercept",
    "text": "MCMC traces: intercept\n\nas.array(brm_2) %&gt;%\n  mcmc_trace(\"b_Intercept\", np = nuts_params(brm_2))"
  },
  {
    "objectID": "slides/02_diagnostics.html#mcmc-traces-isword",
    "href": "slides/02_diagnostics.html#mcmc-traces-isword",
    "title": "Introduction to Bayesian Linear Models",
    "section": "MCMC traces: IsWord",
    "text": "MCMC traces: IsWord\n\nas.array(brm_2) %&gt;%\n  mcmc_trace(\"b_IsWordFALSE\", np = nuts_params(brm_2))"
  },
  {
    "objectID": "slides/02_diagnostics.html#hatr-and-effective-sample-size",
    "href": "slides/02_diagnostics.html#hatr-and-effective-sample-size",
    "title": "Introduction to Bayesian Linear Models",
    "section": "\\(\\hat{R}\\) and Effective Sample Size",
    "text": "\\(\\hat{R}\\) and Effective Sample Size\n\nbrm_2\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ IsWord \n   Data: mald (Number of observations: 3000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     981.35      8.73   964.38   998.37 1.00     3970     3032\nIsWordFALSE   132.87     12.63   107.91   156.87 1.00     3936     2937\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma   341.21      4.48   332.33   349.82 1.00     4223     3006\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "slides/02_diagnostics.html#hatr-and-effective-sample-size-ess",
    "href": "slides/02_diagnostics.html#hatr-and-effective-sample-size-ess",
    "title": "Introduction to Bayesian Linear Models",
    "section": "\\(\\hat{R}\\) and Effective Sample Size (ESS)",
    "text": "\\(\\hat{R}\\) and Effective Sample Size (ESS)\n\nbrm_2\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: RT ~ IsWord \n   Data: mald (Number of observations: 3000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     981.35      8.73   964.38   998.37 1.00     3970     3032\nIsWordFALSE   132.87     12.63   107.91   156.87 1.00     3936     2937\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma   341.21      4.48   332.33   349.82 1.00     4223     3006\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "slides/02_diagnostics.html#posterior-predictive-checks",
    "href": "slides/02_diagnostics.html#posterior-predictive-checks",
    "title": "Introduction to Bayesian Linear Models",
    "section": "Posterior Predictive Checks",
    "text": "Posterior Predictive Checks\n\npp_check(brm_2, ndraws = 20)"
  },
  {
    "objectID": "slides/02_diagnostics.html#summary",
    "href": "slides/02_diagnostics.html#summary",
    "title": "Introduction to Bayesian Linear Models",
    "section": "Summary",
    "text": "Summary\n\nQuick and dirty diagnostics:\n\nMCMC traces: hairy caterpillars, no divergent transitions.\n$: should be as close to 1 as possible.\nEffective Sample Size (ESS): should be large enough.\nPosterior Predictive Checks: predicted outcome distribution should match the empirical distribution.\n\n\n\n\n\nbrm() warns you about divergent transition, \\(\\hat{R} &gt; 1\\) and low ESS.\nPosterior predictive checks are based on visual inspection only."
  }
]